{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25b1e2e",
   "metadata": {},
   "source": [
    "Based on https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec41400b-e416-4297-b266-36b44539d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f5d6ca-6f17-4931-aac9-26aa1f933622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, prev=(), op=None, *args, **kwargs):\n",
    "        self.data = data\n",
    "        self.prev = prev\n",
    "        self.grad = 0\n",
    "        self.op = op\n",
    "        self.grad_fn = lambda x: None\n",
    "        self.broadcast_dim = None\n",
    "    \n",
    "    def backward(self, gradient=None):\n",
    "        if gradient is None:\n",
    "            gradient = np.ones_like(self.data)\n",
    "        self.grad = gradient\n",
    "        self.grad_fn(self.grad)\n",
    "        for p in self.prev:\n",
    "            p.backward(p.grad)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.data)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        self.checkbroadcast(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), op='+')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient if self.broadcast_dim is None else gradient.sum(self.broadcast_dim)\n",
    "            other.grad += gradient if other.broadcast_dim is None else gradient.sum(other.broadcast_dim)\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, (self, other), op='*')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * other.data\n",
    "            other.grad += gradient * self.data\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Tensor(self.data ** other, (self,), op='*')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * (other * (self.data ** (other-1)))\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        out = Tensor(self.data @ other.data, (self, other), op='@')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient @ other.data.T\n",
    "            other.grad += self.data.T @ gradient\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(self.data*(self.data>0), (self,), op='relu')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * (out.data > 0)\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def checkbroadcast(self, other):\n",
    "        for n,(i,j) in enumerate(zip(self.shape, other.shape)):\n",
    "            if i==j:\n",
    "                continue\n",
    "            if i<j:\n",
    "                self.broadcast_dim = n\n",
    "                break\n",
    "            else:\n",
    "                other.broadcast_dim = n\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "59e0e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Tensor(np.ones((4,5),dtype=np.float32)*2)\n",
    "w = Tensor(np.ones((5,4),dtype=np.float32)*3)\n",
    "bi = Tensor(np.ones((4,1),dtype=np.float32)*4)\n",
    "y = Tensor(np.ones((4,4),dtype=np.float32)*20)\n",
    "lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "122999f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136.0\n",
      "1450.0865\n",
      "557.4134\n",
      "214.26971\n",
      "82.36513\n",
      "31.661057\n",
      "12.170502\n",
      "4.6783466\n",
      "1.798372\n",
      "0.6912988\n",
      "0.26573542\n",
      "0.10215093\n",
      "0.039266698\n",
      "0.015093631\n",
      "0.0058021545\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    o = inp @ w + bi\n",
    "    loss = (o - y)**2\n",
    "    print(loss.data.sum())\n",
    "#     w.grad = bi.grad = 0\n",
    "    loss.backward()\n",
    "    w, bi = [x - lr*x.grad for x in [w,bi]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0757227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.019043, 20.019043, 20.019043, 20.019043],\n",
       "       [20.019043, 20.019043, 20.019043, 20.019043],\n",
       "       [20.019043, 20.019043, 20.019043, 20.019043],\n",
       "       [20.019043, 20.019043, 20.019043, 20.019043]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc6714b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d6346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "74e94a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9b0e0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.ones((4,5))*2.)\n",
    "b = torch.tensor(np.ones((5,4))*3., requires_grad=True)\n",
    "d = torch.tensor(np.ones((1,4))*4., requires_grad=True)\n",
    "f = torch.tensor(np.ones((4,4))*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9d88a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3136., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(1450.0864, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(670.5200, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(310.0484, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(143.3664, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(66.2926, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(30.6537, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(14.1743, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(6.5542, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(3.0307, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(1.4014, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(0.6480, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(0.2996, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(0.1386, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor(0.0641, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    e = a @ b + d\n",
    "    loss = (e - f)**2\n",
    "    print(loss.sum())\n",
    "    loss.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        b -= lr*b.grad\n",
    "        d -= lr*d.grad\n",
    "        b.grad = None\n",
    "        d.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30f063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
