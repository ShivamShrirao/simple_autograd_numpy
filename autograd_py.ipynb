{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637149c8",
   "metadata": {},
   "source": [
    "Based on https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec41400b-e416-4297-b266-36b44539d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f5d6ca-6f17-4931-aac9-26aa1f933622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, prev=(), op=None, *args, **kwargs):\n",
    "        self.data = data\n",
    "        self.prev = prev\n",
    "        self.grad = 0\n",
    "        self.op = op\n",
    "        self.grad_fn = lambda x: None\n",
    "        self.broadcast_dim = None\n",
    "    \n",
    "    def backward(self, gradient=None):\n",
    "        if gradient is None:\n",
    "            gradient = np.ones_like(self.data)\n",
    "        self.grad = gradient\n",
    "        self.grad_fn(self.grad)\n",
    "        for p in self.prev:\n",
    "            p.backward(p.grad)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.data)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        self.checkbroadcast(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), op='+')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient if self.broadcast_dim is None else gradient.sum(self.broadcast_dim)\n",
    "            other.grad += gradient if other.broadcast_dim is None else gradient.sum(other.broadcast_dim)\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, (self, other), op='*')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * other.data\n",
    "            other.grad += gradient * self.data\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Tensor(self.data ** other, (self,), op='*')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * (other * (self.data ** (other-1)))\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        out = Tensor(self.data @ other.data, (self, other), op='@')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient @ other.data.T\n",
    "            other.grad += self.data.T @ gradient\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(self.data*(self.data>0), (self,), op='relu')\n",
    "        def grad_fn(gradient):\n",
    "            self.grad += gradient * (out.data > 0)\n",
    "        out.grad_fn = grad_fn\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def checkbroadcast(self, other):\n",
    "        for n,(i,j) in enumerate(zip(self.shape, other.shape)):\n",
    "            if i==j:\n",
    "                continue\n",
    "            if i<j:\n",
    "                self.broadcast_dim = n\n",
    "                break\n",
    "            else:\n",
    "                other.broadcast_dim = n\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee60cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Tensor(np.ones((4,5),dtype=np.float32)*2.)\n",
    "w = Tensor(np.ones((5,4),dtype=np.float32)*-3.)\n",
    "bi = Tensor(np.ones((4,1),dtype=np.float32)*4.)\n",
    "y = Tensor(np.ones((4,4),dtype=np.float32)*20.)\n",
    "lr=1e-2\n",
    "params = (w,bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139edf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33856.0\n",
      "15655.013\n",
      "6017.789\n",
      "2313.2378\n",
      "889.20825\n",
      "341.81174\n",
      "131.39244\n",
      "50.507313\n",
      "19.41504\n",
      "7.4631276\n",
      "2.8688097\n",
      "1.1027628\n",
      "0.42389166\n",
      "0.16294433\n",
      "0.0626374\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    o = inp @ params[0] + params[1]\n",
    "    loss = (o - y)**2\n",
    "    print(loss.data.sum())\n",
    "#     w.grad = bi.grad = 0\n",
    "    loss.backward()\n",
    "    params = [x - lr*x.grad for x in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8144ba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.937431, 19.937431, 19.937431, 19.937431],\n",
       "       [19.937431, 19.937431, 19.937431, 19.937431],\n",
       "       [19.937431, 19.937431, 19.937431, 19.937431],\n",
       "       [19.937431, 19.937431, 19.937431, 19.937431]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401ef8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.],\n",
       "       [20., 20., 20., 20.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415efc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd888e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e8425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.ones((4,5),dtype=np.float32)*2.)\n",
    "b = torch.tensor(np.ones((5,4),dtype=np.float32)*-3., requires_grad=True)\n",
    "d = torch.tensor(np.ones((1,4),dtype=np.float32)*4., requires_grad=True)\n",
    "f = torch.tensor(np.ones((4,4),dtype=np.float32)*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1409b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33856., grad_fn=<SumBackward0>)\n",
      "tensor(15655.0137, grad_fn=<SumBackward0>)\n",
      "tensor(7238.8794, grad_fn=<SumBackward0>)\n",
      "tensor(3347.2573, grad_fn=<SumBackward0>)\n",
      "tensor(1547.7716, grad_fn=<SumBackward0>)\n",
      "tensor(715.6899, grad_fn=<SumBackward0>)\n",
      "tensor(330.9349, grad_fn=<SumBackward0>)\n",
      "tensor(153.0243, grad_fn=<SumBackward0>)\n",
      "tensor(70.7585, grad_fn=<SumBackward0>)\n",
      "tensor(32.7187, grad_fn=<SumBackward0>)\n",
      "tensor(15.1291, grad_fn=<SumBackward0>)\n",
      "tensor(6.9957, grad_fn=<SumBackward0>)\n",
      "tensor(3.2348, grad_fn=<SumBackward0>)\n",
      "tensor(1.4958, grad_fn=<SumBackward0>)\n",
      "tensor(0.6917, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    e = a @ b + d\n",
    "    loss = (e - f)**2\n",
    "    print(loss.sum())\n",
    "    loss.sum().backward()\n",
    "    with torch.no_grad():\n",
    "        b -= lr*b.grad\n",
    "        d -= lr*d.grad\n",
    "        b.grad = None\n",
    "        d.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e76a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19.7921, 19.7921, 19.7921, 19.7921],\n",
       "        [19.7921, 19.7921, 19.7921, 19.7921],\n",
       "        [19.7921, 19.7921, 19.7921, 19.7921],\n",
       "        [19.7921, 19.7921, 19.7921, 19.7921]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7ea13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20., 20., 20., 20.],\n",
       "        [20., 20., 20., 20.],\n",
       "        [20., 20., 20., 20.],\n",
       "        [20., 20., 20., 20.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8198af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
